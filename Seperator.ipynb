{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement open-unmix (from versions: none)\n",
      "ERROR: No matching distribution found for open-unmix\n"
     ]
    }
   ],
   "source": [
    "!pip install open-unmix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import openunmix\n",
    "\n",
    "# Load pre-trained model\n",
    "separator = openunmix.umxl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\anand/.cache\\torch\\hub\\sigsep_open-unmix-pytorch_master\n"
     ]
    }
   ],
   "source": [
    "separator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxl', device='cpu')  # Change 'cpu' to 'cuda' if you have a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted D:\\Tag\\Vocal Gender Classifier\\Test Songs\\Megham Karukatha.mp3 to D:\\Tag\\Vocal Gender Classifier\\Test Songs\\Megham Karukatha.wav\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Function to convert mp3 to wav\n",
    "def convert_mp3_to_wav(mp3_file, wav_file):\n",
    "    # Load the mp3 file\n",
    "    audio = AudioSegment.from_mp3(mp3_file)\n",
    "    \n",
    "    # Export as wav\n",
    "    audio.export(wav_file, format=\"wav\")\n",
    "\n",
    "# Example usage\n",
    "mp3_file_path = r\"D:\\Tag\\Vocal Gender Classifier\\Test Songs\\Megham Karukatha.mp3\"  # Replace with your MP3 file path\n",
    "wav_file_path = r\"D:\\Tag\\Vocal Gender Classifier\\Test Songs\\Megham Karukatha.wav\"   # Replace with your desired WAV file path\n",
    "\n",
    "convert_mp3_to_wav(mp3_file_path, wav_file_path)\n",
    "\n",
    "print(f\"Converted {mp3_file_path} to {wav_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open-Unmix imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import openunmix\n",
    "print(\"Open-Unmix imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded audio shape: torch.Size([2, 12809216]), Sample rate: 44100\n",
      "Audio shape after unsqueeze: torch.Size([1, 2, 12809216])\n",
      "Estimates shape: torch.Size([1, 4, 2, 12809216])\n",
      "Extracted vocals shape: torch.Size([2, 12809216])\n",
      "Extracted accompaniment shape: torch.Size([2, 12809216])\n",
      "Separated vocals saved as 'vocals.wav'.\n",
      "Separated accompaniment saved as 'accompaniment.wav'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import openunmix\n",
    "import torchaudio\n",
    "\n",
    "# Function to separate and save vocals and accompaniment from an audio file\n",
    "def separate_audio(audio_path, output_vocals='vocals.wav', output_accompaniment='accompaniment.wav'):\n",
    "    # Load the pre-trained model\n",
    "    separator = openunmix.umxl()\n",
    "\n",
    "    # Load the audio file and preprocess it\n",
    "    try:\n",
    "        audio, sample_rate = torchaudio.load(audio_path)\n",
    "        print(f\"Loaded audio shape: {audio.shape}, Sample rate: {sample_rate}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Ensure audio has the correct shape (1, C, T)\n",
    "    if audio.dim() == 1:  # mono\n",
    "        audio = audio.unsqueeze(0).unsqueeze(0)  # Shape (1, 1, T)\n",
    "    elif audio.dim() == 2:  # stereo\n",
    "        audio = audio.unsqueeze(0)  # Shape (1, C, T)\n",
    "\n",
    "    print(f\"Audio shape after unsqueeze: {audio.shape}\")\n",
    "\n",
    "    # Perform separation\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        estimates = separator(audio)\n",
    "\n",
    "    # Check the shape of the estimates\n",
    "    print(f\"Estimates shape: {estimates.shape}\")\n",
    "\n",
    "    # Extract the vocal and accompaniment parts\n",
    "    if estimates.dim() == 3:\n",
    "        # Assuming estimates are in the shape [1, 2, C, T]\n",
    "        vocals = estimates[0, 0]  # First output for vocals\n",
    "        accompaniment = estimates[0, 1]  # Second output for accompaniment\n",
    "    elif estimates.dim() == 4 and estimates.shape[1] >= 2:\n",
    "        # If estimates are in the shape [1, 4, 2, T] (two sources per estimate)\n",
    "        vocals = estimates[0, 0]  # First channel corresponds to vocals\n",
    "        accompaniment = estimates[0, 1]  # Second channel corresponds to accompaniment\n",
    "    else:\n",
    "        print(\"Estimation did not return expected shape for vocals and accompaniment. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Check and print the shape of the extracted vocals and accompaniment\n",
    "    print(f\"Extracted vocals shape: {vocals.shape}\")\n",
    "    print(f\"Extracted accompaniment shape: {accompaniment.shape}\")\n",
    "\n",
    "    # Ensure we save 2D tensors\n",
    "    if vocals.dim() == 3:  # If the vocals tensor is still 3D\n",
    "        vocals = vocals.squeeze(0)  # Reduce to [C, T]\n",
    "\n",
    "    if accompaniment.dim() == 3:  # If the accompaniment tensor is still 3D\n",
    "        accompaniment = accompaniment.squeeze(0)  # Reduce to [C, T]\n",
    "\n",
    "    # Save the vocal audio file\n",
    "    try:\n",
    "        torchaudio.save(output_vocals, vocals, sample_rate=sample_rate)  # Save the extracted vocals\n",
    "        print(f\"Separated vocals saved as '{output_vocals}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving vocal audio file: {e}\")\n",
    "\n",
    "    # Save the accompaniment audio file\n",
    "    try:\n",
    "        torchaudio.save(output_accompaniment, accompaniment, sample_rate=sample_rate)  # Save the extracted accompaniment\n",
    "        print(f\"Separated accompaniment saved as '{output_accompaniment}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving accompaniment audio file: {e}\")\n",
    "\n",
    "# Specify the path to your audio file\n",
    "audio_file_path = r'D:\\Tag\\Vocal Gender Classifier\\Test Songs\\Megham Karukatha.wav'  # Replace with your audio file\n",
    "\n",
    "# Call the separation function\n",
    "separate_audio(audio_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
